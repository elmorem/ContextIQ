# Database Migrations Guide

This guide explains how to manage database schema changes using Alembic migrations in ContextIQ.

## Overview

ContextIQ uses **Alembic** for database schema versioning and migrations. All migrations are managed centrally in the `alembic/` directory.

## Quick Start

### 1. Initialize Database

Create the database and install required extensions:

```bash
# Using script
python scripts/db_init.py

# Or using Make
make db-init
```

This will:
- Create the `contextiq` database if it doesn't exist
- Install required PostgreSQL extensions (uuid-ossp, pgcrypto)
- Verify database connectivity

### 2. Run Migrations

Apply all pending migrations to upgrade to the latest schema:

```bash
# Using script
python scripts/db_migrate.py upgrade

# Or using Make
make db-upgrade
```

### 3. Check Current State

See which migration version the database is at:

```bash
# Using script
python scripts/db_migrate.py current

# Or using Make
make db-current
```

## Creating Migrations

### Autogenerate from Models

The recommended way to create migrations is to let Alembic autogenerate them from your SQLAlchemy models:

```bash
# Using script
python scripts/db_migrate.py create "add user profile fields"

# Or using Make
make db-create MESSAGE="add user profile fields"
```

This will:
1. Compare your SQLAlchemy models with the current database schema
2. Generate a migration file with the necessary changes
3. Place it in `alembic/versions/` with a timestamped filename

**Important**: Always review autogenerated migrations before applying them!

### Create Empty Migration

For custom migrations (data migrations, manual schema changes):

```bash
python scripts/db_migrate.py create "migrate user data" --no-autogenerate
```

Then edit the generated file in `alembic/versions/` to add your custom upgrade/downgrade logic.

## Migration Operations

### Upgrade Database

```bash
# Upgrade to latest (head)
python scripts/db_migrate.py upgrade
make db-upgrade

# Upgrade to specific revision
python scripts/db_migrate.py upgrade abc123
```

### Downgrade Database

```bash
# Downgrade one revision
python scripts/db_migrate.py downgrade -1

# Downgrade to specific revision
python scripts/db_migrate.py downgrade xyz789

# Downgrade to base (empty database)
python scripts/db_migrate.py downgrade base
make db-downgrade
```

### View Migration History

```bash
# Show all migrations
python scripts/db_migrate.py history
make db-history

# Show detailed history
python scripts/db_migrate.py history --verbose
```

### Show Current Revision

```bash
python scripts/db_migrate.py current
make db-current
```

### Show Head Revisions

```bash
python scripts/db_migrate.py heads
```

### Stamp Database

Mark the database as being at a specific revision without running migrations:

```bash
python scripts/db_migrate.py stamp head
```

This is useful when:
- Importing an existing database
- Recovering from migration issues
- Setting up a fresh database that matches your models

## Migration File Structure

Migration files are located in `alembic/versions/` with this naming format:

```
YYYYMMDD_HHMM_<revision>_<slug>.py
```

Example:
```
20241211_1430_abc123def456_add_user_profile_fields.py
```

Each migration file contains:

```python
"""add user profile fields

Revision ID: abc123def456
Revises: xyz789abc123
Create Date: 2024-12-11 14:30:00.000000
"""

from alembic import op
import sqlalchemy as sa

# revision identifiers
revision = 'abc123def456'
down_revision = 'xyz789abc123'
branch_labels = None
depends_on = None


def upgrade() -> None:
    """Apply migration."""
    op.add_column('users', sa.Column('bio', sa.Text(), nullable=True))
    op.add_column('users', sa.Column('avatar_url', sa.String(255), nullable=True))


def downgrade() -> None:
    """Revert migration."""
    op.drop_column('users', 'avatar_url')
    op.drop_column('users', 'bio')
```

## Common Migration Patterns

### Adding a Column

```python
def upgrade() -> None:
    op.add_column('sessions', sa.Column('metadata', sa.JSON(), nullable=True))

def downgrade() -> None:
    op.drop_column('sessions', 'metadata')
```

### Dropping a Column

```python
def upgrade() -> None:
    op.drop_column('sessions', 'old_field')

def downgrade() -> None:
    op.add_column('sessions', sa.Column('old_field', sa.String(100), nullable=True))
```

### Creating an Index

```python
def upgrade() -> None:
    op.create_index('idx_sessions_user_id', 'sessions', ['user_id'])

def downgrade() -> None:
    op.drop_index('idx_sessions_user_id', 'sessions')
```

### Adding a Foreign Key

```python
def upgrade() -> None:
    op.create_foreign_key(
        'fk_memories_session_id',
        'memories', 'sessions',
        ['session_id'], ['id'],
        ondelete='CASCADE'
    )

def downgrade() -> None:
    op.drop_constraint('fk_memories_session_id', 'memories', type_='foreignkey')
```

### Data Migration

```python
from sqlalchemy import table, column, String, Integer

def upgrade() -> None:
    # Reference table for data migration
    sessions = table(
        'sessions',
        column('id', String),
        column('status', String),
    )

    # Update existing data
    op.execute(
        sessions.update()
        .where(sessions.c.status == 'old_status')
        .values(status='new_status')
    )

def downgrade() -> None:
    sessions = table(
        'sessions',
        column('id', String),
        column('status', String),
    )

    op.execute(
        sessions.update()
        .where(sessions.c.status == 'new_status')
        .values(status='old_status')
    )
```

## Makefile Commands

For convenience, common migration commands are available via Make:

```bash
make db-init        # Initialize database and extensions
make db-create      # Create new migration (requires MESSAGE="...")
make db-upgrade     # Upgrade to latest
make db-downgrade   # Downgrade to base
make db-current     # Show current revision
make db-history     # Show migration history
make db-reset       # Reset database (downgrade to base, then upgrade to head)
```

## Best Practices

### 1. Always Review Autogenerated Migrations

Autogeneration is smart but not perfect. Always review generated migrations to ensure:
- All changes are captured correctly
- No unintended changes are included
- Indexes are created appropriately
- Foreign key constraints are handled properly

### 2. Test Migrations Both Ways

Always test both `upgrade()` and `downgrade()`:

```bash
# Test upgrade
python scripts/db_migrate.py upgrade

# Test downgrade
python scripts/db_migrate.py downgrade -1

# Upgrade again
python scripts/db_migrate.py upgrade
```

### 3. Keep Migrations Small and Focused

- One migration per logical change
- Avoid combining multiple unrelated changes
- Use descriptive migration messages

### 4. Handle Data Carefully

For migrations that modify or delete data:
- Backup data first
- Test on a copy of production data
- Consider creating a data migration script separately

### 5. Don't Modify Existing Migrations

Once a migration is committed and shared:
- Never modify it
- Create a new migration to fix issues
- Use `alembic stamp` to recover from issues if needed

### 6. Use Transactions

Migrations run in transactions by default. For operations that can't run in transactions (like creating ENUM types in PostgreSQL), use:

```python
def upgrade() -> None:
    # Run outside transaction
    op.execute("CREATE TYPE status_enum AS ENUM ('active', 'inactive')")
```

## Production Deployments

### Pre-Deployment Checklist

- [ ] All migrations tested locally
- [ ] Database backed up
- [ ] Migration `upgrade()` and `downgrade()` both tested
- [ ] Reviewed for breaking changes
- [ ] Coordinated with application deployment

### Deployment Process

1. **Backup database**:
   ```bash
   pg_dump -h localhost -U contextiq contextiq > backup_$(date +%Y%m%d_%H%M%S).sql
   ```

2. **Apply migrations**:
   ```bash
   python scripts/db_migrate.py upgrade
   ```

3. **Verify application**:
   - Check application logs
   - Verify critical endpoints
   - Monitor error rates

4. **Rollback if needed**:
   ```bash
   python scripts/db_migrate.py downgrade -1
   ```

### Zero-Downtime Migrations

For large tables or complex changes:

1. **Add new columns/tables** (compatible with old code)
2. **Deploy application** that writes to both old and new
3. **Migrate data** in background
4. **Deploy application** that reads from new
5. **Remove old columns/tables** in next migration

## Troubleshooting

### "Multiple heads" Error

If you have branching migrations:

```bash
# Show all heads
python scripts/db_migrate.py heads

# Merge heads by creating a new migration
alembic merge -m "merge heads" head1 head2
```

### Migration Fails Midway

If a migration fails partway through:

```bash
# Check current state
python scripts/db_migrate.py current

# Fix the issue manually if needed
psql -h localhost -U contextiq contextiq

# Stamp the database at the correct revision
python scripts/db_migrate.py stamp <revision>
```

### "Target database is not up to date"

Your database is behind the expected version:

```bash
# Check current version
python scripts/db_migrate.py current

# Check history
python scripts/db_migrate.py history

# Upgrade to latest
python scripts/db_migrate.py upgrade
```

### Can't Connect to Database

```bash
# Verify database settings in .env
cat .env | grep DATABASE

# Test connection manually
psql -h localhost -U contextiq -d contextiq

# Reinitialize if needed
python scripts/db_init.py
```

## Configuration

Database settings are loaded from environment variables:

```bash
# .env file
DATABASE_URL=postgresql+asyncpg://contextiq:password@localhost:5432/contextiq

# Or individual components
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=contextiq
DATABASE_USER=contextiq
DATABASE_PASSWORD=password
```

Alembic configuration is in `alembic.ini`:
- Script location
- File naming template
- Post-write hooks (black, ruff)
- Logging configuration

## References

- [Alembic Documentation](https://alembic.sqlalchemy.org/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [ContextIQ Database Settings](../shared/config/database.py)
- [Migration Scripts](../scripts/)
